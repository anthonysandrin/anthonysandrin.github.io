<html>
<head>
    <title>Anthony Sandrin</title>
    <meta name='description' content='Project'>
    <link rel='stylesheet' href='css/cs488.css'>
    <link rel='stylesheet' href='css/drift-basic.css'>
    <link href='http://fonts.googleapis.com/css?family=Lato|Noto+Serif' rel='stylesheet' type='text/css'>
    <script src="js/Drift.min.js"></script>
    <script type="text/javascript" async
      src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML">
    </script>
    <script async="" src="//www.google-analytics.com/analytics.js"></script>
    <script>
      (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
      (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
      m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
      })(window,document,'script','//www.google-analytics.com/analytics.js','ga');
      ga('create', 'UA-58577506-1', 'auto');
      ga('send', 'pageview');
    </script>
</head>
<body>
  <main>
    <header>
      <a style="color:#000000; text-decoration:none" href="/">Anthony Sandrin</a>
      <h1>Atlas</h1>
      <h2>My final project for CS488</h2>
    </header>

    <div class="hrule"></div>

    <section class="feature">
      <div class="feature-body">
        <div class="feature-details">
          <p>
          Atlas is a path tracer I implemented as my final project for the CS488: Computer Graphics course
          at the University of Waterloo. Named after the focus of my final scene, this renderer implements
          birdiectional path tracing with volumetric scattering along with a number of other features described below.
          </p>
        </div>
      </div>
    </section>

    <section class="gallery">
      <div class="gallery-image">
        <img class="zooming-image" src="img/statue.png"></img>
        <p>Final Scene.</p>
      </div>
    </section>

    <section class="feature">
      <div class="feature-body">
        <div class="feature-header">
          <h2>Path Tracing</h2>
        </div>
        <div class="hrule"></div>
        <div class="feature-details">
          <p>
          I implemented bidirectional path tracing.
          </p>
          <p>
          For each sample two paths
          are generated: a camera path and a light path. The camera path starts
          with a camera ray, while the light path starts with a randomly selected light
          source and direction. The paths are built by repeatedly intersecting rays with
          the scene, recording the intersection point, and creating a new ray based
          on the material of the intersected primitive.
          </p>
          <p>
          After both paths are generated, each camera vertex is connected to each light
          vertex to create many camera-light paths.
          The light throughput of each path \(P\) is calculated using the following equation:
          </p>
          <p class="math">
          \(P(s_1\rightarrow s_2\rightarrow \cdots\rightarrow  s_n \leftrightarrow t_m \leftarrow  \cdots \leftarrow  t_2 \leftarrow t_1) \\
          = C(s_n) BRDF(s_{n-1} \rightarrow s_n \rightarrow t_m) G(s_n\leftrightarrow t_m) BRDF(s_n \rightarrow t_m \rightarrow t_{m-1}) C(t_m) L_e(t_1) \)
          </p>
          <p>
          Where \(s_1\rightarrow\cdots \rightarrow s_n\) is the camera path, \(t_1\rightarrow \cdots t_m\) is the light path,
          \(G(s_n\leftrightarrow t_m)\) is an occlusion factor, determined
          by casting a shadow ray, \(L_e(t_1)\) is the direct illumination at \(t_1\),
          and \(C(v)\) is the contribution of the vertex and is calculated as the path is being constructed.
          </p>
          <p>
          \(C(s_n) = \prod_{i=2}^{n-1} BRDF(s_{i-1}\rightarrow s_{i} \rightarrow s_{i+1}) \)
          </p>
        </div>
      </div>
      <div class="feature-image">
        <img class="zooming-image" src="img/three_spheres_point.png"></img>
        <p>Notice the red floor colour bleeding onto the white spheres.</p>
      </div>
    </section>

    <section class="feature">
      <div class="feature-body">
        <div class="feature-header">
          <h2>Soft Shadows</h2>
        </div>
        <div class="hrule"></div>
        <div class="feature-details">
          <p>
          Soft shadows are a direct consequence of bidirectional path tracing
          and spherical light sources. 
          </p>
          <p>
          Spherical light sources are implemented by choosing a random point on the
          surface of the sphere and a random direction as the start of the light path.
          </p>
          <p>
          This works well for scenes with only diffuse objects where the light itself
          is not visible; however, extra effort is done in order for camera rays to be able
          to intersect with the light directly, or for purely specular camera-light paths to be
          possible. To handle these cases, each spherical light source adds a sphere
          primitive to the scene with a special emissive material.
          When a camera path intersects with these emissive spheres,
          this emitted light is added to the output.
          </p>
        </div>
      </div>
      <div class="feature-image">
        <img class="zooming-image" src="img/soft_shadow.png"></img>
      </div>
    </section>

    <section class="gallery">
      <div class="gallery-image">
        <img class="zooming-image" src="img/three_spheres_sky_box.png"></img>
        <p>No direct light, uniform sky lighting only.</p>
      </div>
      <div class="gallery-image">
        <img class="zooming-image" src="img/three_spheres_point.png"></img>
        <p>Point light and uniform sky lighting.</p>
      </div>
      <div class="gallery-image">
        <img class="zooming-image" src="img/three_spheres.png"></img>
        <p>Spherical light and uniform sky lighting.</p>
      </div>
    </section>

    <section class="feature">
      <div class="feature-body">
        <div class="feature-header">
          <h2>Refraction & Fresnel Effect</h2>
        </div>
        <div class="hrule"></div>
        <div class="feature-details">
          <p>
          I implemented a glass-like material that has both reflection and refraction components
          combined together using the Fresnel equations.
          </p>
          <p>
          In the following image, notice the reflections around the edges of the spheres and the caustic
          effects of the spheres on the floor.
          </p>
        </div>
      </div>
      <div class="feature-image">
        <img class="zooming-image" src="img/glass.png"></img>
        <p>Glass spheres.</p>
      </div>
      <div class="feature-image">
        <img class="zooming-image" src="img/caustics_bunny_bdpt.png" style="width:1024px; height:auto"></img>
        <p>Stanford Bunny caustics.</p>
      </div>
    </section>

    <section class="feature">
      <div class="feature-body">
        <div class="feature-header">
          <h2>Texture Mapping</h2>
        </div>
        <div class="hrule"></div>
        <div class="feature-details">
          <p>
          I implemented texture mapping by storing the \(u\) and \(v\) coordinates of each intersection 
          point and sampling textures once using linear interpolation. Antialiasing is achieved by simply
          taking more samples per pixel.
          </p>
        </div>
      </div>
      <div class="feature-image">
        <img class="zooming-image" src="img/trex3.png"></img>
        <p>Textured T. rex and procedural checkerboard texture.</p>
      </div>
    </section>

    <section class="feature">
      <div class="feature-body">
        <div class="feature-header">
          <h2>Bump Mapping</h2>
        </div>
        <div class="hrule"></div>
        <div class="feature-details">
          <p>
          Bump mapping is implemented by storing the normal and \(u\) and \(v\) tangent vectors
          at each intersection point and perturbing the normal according to a sample from 
          a normal map.
          </p>
          <p>
          In the following animation, notice how the shading on the surface of the sphere changes
          as the light moves.
          </p>
        </div>
      </div>
      <div class="feature-image">
        <img class="zooming-image" src="img/bump.gif" style="width:1024px; height:auto"></img>
      </div>
    </section>

    <section class="feature">
      <div class="feature-body">
        <div class="feature-header">
          <h2>Perlin Noise</h2>
        </div>
        <div class="hrule"></div>
        <div class="feature-details">
          <p>I implemented 3D and 4D Perlin noise using Perlin's improved interpolation technique.</p>
          <p>
          The next animation is created by measuring the derivative of the Perlin noise at the surface of
          the sphere with respect to the \(u\) and \(v\) tangent vectors. These values are used to perturb
          the surface normal. I then create an animation using time as the fourth dimension of
          the Perlin noise input.
          </p>
        </div>
      </div>
      <div class="feature-image">
        <img class="zooming-image" src="img/perlin.gif" style="width:1024px; height:auto"></img>
      </div>
    </section>

    <section class="feature">
      <div class="feature-body">
        <div class="feature-header">
          <h2>Depth of Field</h2>
        </div>
        <div class="hrule"></div>
        <div class="feature-details">
          <p>
          I implemented depth of field using the thin lens camera model.
          Each sample starts at a random point on the camera's aperture and is given a direction such that
          all rays for a specific pixel intersect at the same point on the focal plane.
          </p>
        </div>
      </div>
      <div class="feature-image">
        <img class="zooming-image" src="img/dof.png"></img>
      </div>
    </section>

    <section class="feature">
      <div class="feature-body" style="flex: 2">
        <div class="feature-header">
          <h2>Antialiasing</h2>
        </div>
        <div class="hrule"></div>
        <div class="feature-details">
          <p>
          Multisample antialiasing is implemented by sampling rays not at the center of each pixel
          but at a uniformly random point within each pixel.
          </p>
          <p>
          The first image is created by taking samples at the exact center of each pixel while the second
          is created by taking 8 samples randomly distributed within each pixel. In the first image, notice the
          jagged edges on the shadows and along the checkerboard lines. Those same lines are smooth in the
          second image.
          </p>
        </div>
      </div>
      <div class="feature-image">
        <img class="zooming-image" src="img/antialias_bad.png"></img>
        <p>Not antialiased.</p>
      </div>
      <div class="feature-image">
        <img class="zooming-image" src="img/antialias.png"></img>
        <p>Antialiased.</p>
      </div>
    </section>

    <section class="feature">
      <div class="feature-body">
        <div class="feature-header">
          <h2>Acceleration</h2>
        </div>
        <div class="hrule"></div>
        <div class="feature-details">
          <p>
          For acceleration, I implemented axis-aligned bounding box for each primitive and a
          k-d tree for storing the faces of each mesh.
          I also implemented multithreading by having each thread
          write to a thread-local buffers, accumulating all the results at the end.
          </p>
          <p>
          The following scene with the Stanford dragon model contains ~100,000 faces. I rendered 
          the scene at 512x512 resolution and with 8 samples per pixel.
          I did my performance benchmarking on a 64 core Google Cloud Intel Skylake compute-optimized machine.
          </p>
          <p>
          Without a mesh k-d tree and on a single thread it would take ~420 minutes to render the image.
          With a mesh k-d tree on a single thread it takes only 288 seconds -- An 87x improvement!.
          </p>
          <p>
          The following charts show the multicore scaling with mesh k-d tree enabled.
          Up to 32 cores, the scaling is nearly perfect but drops off to 75% scaling at 64 cores.
          Using 64 cores and a k-d tree the scene can be rendered in just 5.95 seconds
          -- A 4000x improvement over no acceleration!
          </p>
<iframe width="710" height="445" seamless frameborder="0" scrolling="no" src="https://docs.google.com/spreadsheets/d/e/2PACX-1vQdzS0hpzMBCbBJesps8kFxsLqzNfxUrb0VGljQaMg8PCtN6jTEenRys4ycgiuhcYRiR3jX3q1bFj8y/pubchart?oid=594922133&amp;format=interactive"></iframe>
<iframe width="710" height="445" seamless frameborder="0" scrolling="no" src="https://docs.google.com/spreadsheets/d/e/2PACX-1vQdzS0hpzMBCbBJesps8kFxsLqzNfxUrb0VGljQaMg8PCtN6jTEenRys4ycgiuhcYRiR3jX3q1bFj8y/pubchart?oid=584169773&amp;format=interactive"></iframe>
        </div>
      </div>
      <div class="feature-image">
        <img class="zooming-image" src="img/dragon.png"></img>
      </div>
    </section>

    <section class="feature">
      <div class="feature-body">
        <div class="feature-header">
          <h2>Glossy Reflections</h2>
        </div>
        <div class="hrule"></div>
        <div class="feature-details">
          <p>
          I added glossy reflections to my material model by adding a reflectiveness
          parameter and a glossiness parameter.
          The reflectiveness parameter performs a linear interpolation between
          a Lambertian surface model and a perfect reflection model.
          The glossiness parameter that controls a random perturbation of the
          reflected ray and how sharp the BRDF is at the reflected angle.
          </p>
        </div>
      </div>
      <div class="feature-image">
        <img class="zooming-image" src="img/glossy2.png"></img>
        <p>
        Three spheres with varying glossiness.
        </p>
      </div>
      <div class="feature-image">
        <img class="zooming-image" src="img/glossy.png"></img>
        <p>
        Textured materials with glossy reflections.
        </p>
      </div>
    </section>

    <section class="feature">
      <div class="feature-body">
        <div class="feature-header">
          <h2>Metropolis Light Transport</h2>
        </div>
        <div class="hrule"></div>
        <div class="feature-details">
          <p>
          I implemented pure sample space Metropolis light transport.
          </p>
          <p>
          Each path is identified with the vector of random numbers
          used to generate it. By applying random mutations to this
          vector of numbers, a Markov chain of paths can be created.
          This Markov chain is then used as part of the Metropolis sampling
          algorithm to sample paths with probability density approximately
          equal to the luminance of the path.
          </p>
          <p>
          I implemented two mutation types:
          a large step that completely randomizes the random vector and
          a small step that applies a small normally distributed perturbation
          to the vector.
          </p>
          <p>
          This implementation still needs some tweaking to be usable and
          in general produces noisier results than standard bidirectional path tracing.
          To improve the results I would need to reduce startup bias by adopting
          a better approach for generating bootstrap samples. I could also
          implement a more sophisticated mutation strategy.
          </p>
        </div>
      </div>
    </section>

    <section class="gallery">
      <div class="gallery-image">
        <img class="zooming-image" src="img/caustics_bdpt.png"></img>
        <p>Standard BDPT, 32 Samples.</p>
      </div>
      <div class="gallery-image">
        <img class="zooming-image" src="img/caustics_mlt.png"></img>
        <p>Metropolis Light Transport, 32 Samples.</p>
      </div>
      <div class="gallery-image">
        <img class="zooming-image" src="img/caustics_weight.png"></img>
        <p>Metropolis Light Transport samples per pixel.</p>
      </div>
    </section>

    <section class="feature">
      <div class="feature-body">
        <div class="feature-header">
          <h2>Volumetric Scattering</h2>
        </div>
        <div class="hrule"></div>
        <div class="feature-details">
          <p>
          I implemented volumetric scattering by assigning each object in the
          scene an internal medium and also designating a global scene medium.
          </p>
          As paths are being constructed, I keep track of the medium that the current
          ray is travelling through. Before adding a vertex to the path, I first
          consider the possibility that the light will scatter before reaching the
          intersection point. In this case a special scattering
          vertex is added to the path instead of the usual intersection vertex.
          </p>
          <p>
          To calculating the throughput of each path, the equation described
          above is modified to be:
          </p>
          <p class="math">
          \(P(s_1\rightarrow s_2\rightarrow \cdots\rightarrow  s_n \leftrightarrow t_m \leftarrow  \cdots \leftarrow  t_2 \leftarrow t_1) \\
          = C(s_n) f(s_{n-1} \rightarrow s_n \rightarrow t_m) T(s_n\leftarrow t_m) f(s_n \rightarrow t_m \rightarrow t_{m-1}) C(t_m) L_e(t_1) \)
          </p>
          <p>
          Where the occlusion factor \(G(s_n\leftrightarrow t_m)\) has been replaced by a transmission factor
          \(T(s_n\leftarrow t_m)\), \(f(\cdots)\) is either the BRDF of the material or the phase function of the scattering medium
          and \(C(v)\) has been modified such that
          </p>
          <p>
          \(C(s_n) = (\prod_{i=2}^{n-1} f(s_{i-1}\rightarrow s_{i} \rightarrow s_{i+1})T(s_{i-1}\rightarrow s_{i})) T(s_{n-1}\rightarrow s_n) \)
          </p>
        </div>
      </div>
    </section>

    <section class="gallery">
      <div class="gallery-image">
        <img class="zooming-image" src="img/medium.png"></img>
        <p>
        Homogeneous global medium.
        <p>
      </div>
      <div class="gallery-image">
        <img class="zooming-image" src="img/perlin_medium.png"></img>
        <p>
        Medium contained within a sphere using ray marching over Perlin noise.
        <p>
      </div>
      <div class="gallery-image">
        <img class="zooming-image" src="img/earth.png"></img>
        <p>
        Clouds rendered using ray marching on NASA's blue marble imagery.
        <p>
      </div>
    </section>

    <section class="feature">
      <div class="feature-body">
        <div class="feature-header">
          <h2>Final Scene</h2>
        </div>
        <div class="hrule"></div>
        <div class="feature-details">
          <p>
          My final scene is inspired by Greek Mythology. After the war of the titans, 
          Zeus condemns the titan Atlas to hold up the sky. This is my 
          interpretation of that myth.
          </p>
          <p>
          To render the "sky", I used a distorted version of the ray-marching medium
          using NASA's "Blue Marble" imagery.
          The god-ray effect is caused by the rings of the statue casting shadows
          through a homogeneous global medium.
          </p>
          <p>
          The final scene is rendered at 1920x1080 with 2048 samples per pixel.
          It took ~11 hours to render on a 64 core Google cloud compute machine.
          </p>
        </div>
      </div>
      <div class="feature-image">
        <img class="zooming-image" src="img/concept.png"></img>
        <p>Final scene concept sketch.</p>
      </div>
    </section>
    <section class="gallery">
      <div class="gallery-image">
        <img class="zooming-image" src="img/statue.png"></img>
        <p>Final Scene.</p>
      </div>
    </section>

    <section class="feature">
      <div class="feature-body">
        <div class="feature-header">
          <h2>Credits</h2>
        </div>
        <div class="hrule"></div>
        <div class="feature-details">
          <p>
          Credit to Greg Zaal and <a href="https://hdrihaven.com">hdrihaven.com</a> for the hdri skyboxes used in some of the renders above.
          </p>
          <p>
          Credit to joel3d at <a href="https://www.turbosquid.com/FullPreview/Index.cfm/ID/933905">turbosquid.com</a> for the T. rex model used
          in the texture mapping render.
          </p>
          <p>
          Credit to CG_Luke at <a href="https://www.turbosquid.com/FullPreview/Index.cfm/ID/1049650">turbosquid.com</a> for the human base model used
          in the final scene.
          </p>
          <p>
          Credit to the <a href="http://graphics.stanford.edu/data/3Dscanrep/">Stanford 3D Scanning Repository</a> for the Stanford Bunny and
          Stanford Dragon models.
          </p>
          <p>
          Credit to <a href="https://3dtextures.me">3dtextures.me</a> for the remaining textures used.
          </p>
          <p>
          Credit to NASA for the <a href="https://visibleearth.nasa.gov/view.php?id=57723">Blue Marble</a> imagery.
          </p>
          <p>
          Credit to <a href="https://www.graphics.cornell.edu/~bjw/rgbe.html">Greg Ward</a> for his implementation of the RGBE file format.
          </p>
          <p>
          I used the <a href="https://github.com/ThePhD/sol2">sol</a> library
          for extending the lua interface.
          </p>
          <p>
          I referenced the book <a href="https://www.pbrt.org">Physically Based Rendering</a> for much of the theory involved.
          </p>
        </div>
      </div>
    </section>

    <footer>
    </footer>
  </main>
  <script src="js/cs488.js"></script>
</body>
</html>
